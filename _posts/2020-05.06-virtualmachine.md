---
title : "VirtualBox 가상머신 하면서 느낀 해결법 정리"
author : "금주"
#categories : - Study
date: "2020-05-06"
---

호스트 전용 네트워크 생성하기
VBoxManage hostonlyif create

Dns 설정하기
Sudo nano /etc/resolv.conf

https://rogerdudler.github.io/git-guide/index.ko.html

sudo rm /var/lib/ubuntu-release-upgrader/release-upgrade-available


https://qastack.kr/ubuntu/91543/apt-get-update-fails-to-fetch-files-temporary-failure-resolving-error

먼저 알려진 DNS 서버를 시스템에 임시로 추가하십시오.




U
<b> echo "nameserver 8.8.8.8" | sudo tee /etc/resolv.conf > /dev/null  </b>
그런 다음을 실행하십시오 sudo apt-get update.

이렇게하면 임시 해결 메시지가 해결 되면 24 시간 동안 ISP가 문제를 해결하는지 확인하거나 ISP에 문의하십시오. 또는 시스템에 DNS 서버를 영구적으로 추가 할 수 있습니다.

<b>echo "nameserver 8.8.8.8" | sudo tee /etc/resolvconf/resolv.conf.d/base > /dev/null</b>
8.8.8.8 Google의 자체 DNS 서버입니다.


hadoop fs -rmr /Receipts/output



wget https://github.com/Maki94/kmeans_mapreduce/archive/master.zip
unzip master.zip
sudo mkdir Kmeans


# specify input parameters
JAR_PATH=~/MapReduce/Kmeans/executable_jar/kmeans_mapreduce.jar
MAIN_CLASS=Main
INPUT_FILE_PATH=~/MapReduce/Kmeans/Resources/Input/points.txt
STATE_PATH=~/MapReduce/Kmeans/Resources/Input/clusters.txt
NUMBER_OF_REDUCERS=3
OUTPUT_DIR=~/MapReduce/Kmeans/Resources/Output
DELTA=100000000.0
MAX_ITERATIONS=10
DISTANCE=eucl

hadoop jar ${JAR_PATH} ${MAIN_CLASS} --input ${INPUT_FILE_PATH} \
--state ${STATE_PATH} \
--number ${NUMBER_OF_REDUCERS} \
--output ${OUTPUT_DIR} \
--delta ${DELTA} \
--max ${MAX_ITERATIONS} \
--distance ${DISTANCE}

# execute jar file
LAST_DIR="$(hadoop fs -ls -t -C ~/KMeans/Resources/Output | head -1)"

# print results
hadoop fs -cat "$LAST_DIR/part-r-[0-9][0-9][0-9][0-9][0-9]" | sort --numeric --key 1



# specify input parameters
JAR_PATH=~/MapReduce/Kmeans/executable_jar/kmeans_mapreduce.jar
MAIN_CLASS=Main
INPUT_FILE_PATH=~/MapReduce/Kmeans/Resources/Input/points.txt
STATE_PATH=~/MapReduce/Kmeans/Resources/Input/clusters.txt
NUMBER_OF_REDUCERS=3
OUTPUT_DIR=~/MapReduce/Kmeans/Resources/Output
DELTA=100000000.0
MAX_ITERATIONS=10
DISTANCE=eucl

hadoop jar ${JAR_PATH} ${MAIN_CLASS} --input ${INPUT_FILE_PATH} \
--state ${STATE_PATH} \
--number ${NUMBER_OF_REDUCERS} \
--output ${OUTPUT_DIR} \
--delta ${DELTA} \
--max ${MAX_ITERATIONS} \
--distance ${DISTANCE}

# execute jar file
LAST_DIR="$(hadoop fs -ls -t -C ~/KMeans/Resources/Output | head -1)"

# print results
hadoop fs -cat "$LAST_DIR/part-r-[0-9][0-9][0-9][0-9][0-9]" | sort --numeric --key 1




# specify input parameters
JAR_PATH=~/MapReduce/Kmeans/executable_jar/kmeans_mapreduce.jar
MAIN_CLASS=Main
INPUT_FILE_PATH=/KMeans/Resources/Input/points.txt
STATE_PATH=/KMeans/Resources/Input/clusters.txt
NUMBER_OF_REDUCERS=3
OUTPUT_DIR=/KMeans/Resources/Output
DELTA=100000000.0
MAX_ITERATIONS=10
DISTANCE=eucl

hadoop jar ${JAR_PATH} ${MAIN_CLASS} --input ${INPUT_FILE_PATH} \
--state ${STATE_PATH} \
--number ${NUMBER_OF_REDUCERS} \
--output ${OUTPUT_DIR} \
--delta ${DELTA} \
--max ${MAX_ITERATIONS} \
--distance ${DISTANCE}



스파크 실행: $SPARK_HOME/sbin/start-all.sh
스파크 중지: $ $SPARTK_HOME/sbin/stop-all.sh

하둡 오류 시 아래의 절차로 복구
• 스파크종료:$SPARTK_HOME/sbin/stop-all.sh
• 하둡종료:$stop-all.sh
• 하둡데이터디렉토리삭제:$rm–r$HADOOP_HOME/hdfs/data
• 하둡재시작:$start-all.sh
• 하둡포맷:$hadoopnamenode–format
• 기존의하둡데이터는모두삭제됨
• jar파일복사
$ hadoop fs –mkdir /jar
$ hadoop fs -mkdir /jar/spark-jars
$ hadoop fs –put $SPARK_HOME/jars/* /jar/spark-jars/
• 스파크시작:$SPARK_HOME/sbin/start-all.sh




sudo rm Raffle.scala



sudo nano Raffle.scala
sudo scalac Raffle.scala
scala Raffle.scala

스파크실행
start-all.sh
$SPARK_HOME/sbin/start-all.sh
$ZEPPELIN_HOME/bin/zeppelin-daemon.sh start
$ZEPPELIN_HOME/bin/zeppelin-daemon.sh stop
$SPARK_HOME/sbin/stop-all.sh
Stop-all.sh

$SPARK_HOME/bin/spark-shell --master yarn


SFPD 데이터세트 생성 - 코드

// 클래스 임포트
import spark.implicits._

// 케이스 클래스 정의
case class Incidents(incidentnum:String, category:String, description:String, dayofweek:String, date:String, time:String, pddistrict:String, resolution:String, address:String, X:Double, Y:Double, pdid:String)

// 데이터 적재 후 데이터세트 생성
val sfpdDS=spark.read.option("inferSchema",true).csv("/sparkdata/sfpd/sfpd.csv").toDF("incidentnum","category","description","dayofweek","date","time","pddistrict","resolution","address","X","Y","pdid").as[Incidents]


sfpdDS.printSchema()

// 데이터세트를 뷰(view)로 등록
sfpdDS.createTempView("sfpd")



SFPD 데이터세트 생성 (프로그램) – 코드 (1)

 // 클래스 임포트
import spark.implicits._<br>
import org.apache.spark.sql.types._

<br>
// 스키마 생성
val sfpdSchema = new StructType(Array(
<t>new StructField("incidentnum", StringType, true),
<t>new StructField("category", StringType, true), new StructField("description", <t>StringType, true), new StructField("dayofweek", StringType, true), 
<t>new StructField("date", StringType, true),
<t>new StructField("time", StringType, true),
<t>new StructField("pddistrict", StringType, true), 
<t>new StructField("resolution", StringType, true), new StructField("address", StringType, true),
<t>new StructField("X", FloatType, true), new StructField("Y", FloatType, true),
<t>new StructField("pdid", StringType, true)





데이터를 적재하여 데이터프레임을 생성하는 메서드 • spark.read.load("파일")
• 파케이 파일(디폴트)를 적재<br>
• spark.read.load("파일").format("타입")<br>
• 타입: JSON, 파케이(Parquet). CSV</br>
• spark.read.text("파일")<br> • 텍스트파일을적재<br>
• spark.read.textfile("파일")
• 텍스트 파일을 적재하고, 데이터프레임이 아닌 Dataset[String]을 리턴
• spark.read.jdbc(URL, Table, Connection_Properties) • JDBC 연결을 사용하여 데이터베이스 테이블을 적재

spark.read.csv("파일")
• spark.read.load("파일").format("csv")
spark.read.json("파일")
• spark.read.load("파일").format("json")
spark.read.parquet("파일")
• spark.read.load("파일").format("parquet")

프로그램으로 SFPD 데이터프레임 생성 절차
• 클래스임포트
•spark.implicits와 SQL 데이터타입 클래스들을 임포트 • 프로그램으로스키마생성
•StructField 객체의 배열로 구성된 StructType 객체 생성
• 데이터적재후데이터프레임생성 •spark.read.format 메서드를 사용하여 데이터 적재 •toDF 를 사용하여 데이터프레임을 생성
• 케이스클래스정의
• 열이름과타입이지정된케이스클래스정의
• 케이스클래스를사용하여데이터프레임을데이터세트로변환 • .as[T] 메서드를 사용하여 데이터세트로 변환
• 데이터세트를뷰(view)로등록 • 뷰를사용하여SQL질의
