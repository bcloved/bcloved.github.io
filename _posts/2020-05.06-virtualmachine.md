---
title : "VirtualBox 가상머신 하면서 느낀 해결법 정리"
author : "금주"
#categories : - Study
date: "2020-05-06"
---

호스트 전용 네트워크 생성하기
VBoxManage hostonlyif create

Dns 설정하기
Sudo nano /etc/resolv.conf

https://rogerdudler.github.io/git-guide/index.ko.html

sudo rm /var/lib/ubuntu-release-upgrader/release-upgrade-available


https://qastack.kr/ubuntu/91543/apt-get-update-fails-to-fetch-files-temporary-failure-resolving-error

먼저 알려진 DNS 서버를 시스템에 임시로 추가하십시오.




U
<b> echo "nameserver 8.8.8.8" | sudo tee /etc/resolv.conf > /dev/null  </b>
그런 다음을 실행하십시오 sudo apt-get update.

이렇게하면 임시 해결 메시지가 해결 되면 24 시간 동안 ISP가 문제를 해결하는지 확인하거나 ISP에 문의하십시오. 또는 시스템에 DNS 서버를 영구적으로 추가 할 수 있습니다.

<b>echo "nameserver 8.8.8.8" | sudo tee /etc/resolvconf/resolv.conf.d/base > /dev/null</b>
8.8.8.8 Google의 자체 DNS 서버입니다.


hadoop fs -rmr /Receipts/output



wget https://github.com/Maki94/kmeans_mapreduce/archive/master.zip
unzip master.zip
sudo mkdir Kmeans


# specify input parameters
JAR_PATH=~/MapReduce/Kmeans/executable_jar/kmeans_mapreduce.jar
MAIN_CLASS=Main
INPUT_FILE_PATH=~/MapReduce/Kmeans/Resources/Input/points.txt
STATE_PATH=~/MapReduce/Kmeans/Resources/Input/clusters.txt
NUMBER_OF_REDUCERS=3
OUTPUT_DIR=~/MapReduce/Kmeans/Resources/Output
DELTA=100000000.0
MAX_ITERATIONS=10
DISTANCE=eucl

hadoop jar ${JAR_PATH} ${MAIN_CLASS} --input ${INPUT_FILE_PATH} \
--state ${STATE_PATH} \
--number ${NUMBER_OF_REDUCERS} \
--output ${OUTPUT_DIR} \
--delta ${DELTA} \
--max ${MAX_ITERATIONS} \
--distance ${DISTANCE}

# execute jar file
LAST_DIR="$(hadoop fs -ls -t -C ~/KMeans/Resources/Output | head -1)"

# print results
hadoop fs -cat "$LAST_DIR/part-r-[0-9][0-9][0-9][0-9][0-9]" | sort --numeric --key 1



# specify input parameters
JAR_PATH=~/MapReduce/Kmeans/executable_jar/kmeans_mapreduce.jar
MAIN_CLASS=Main
INPUT_FILE_PATH=~/MapReduce/Kmeans/Resources/Input/points.txt
STATE_PATH=~/MapReduce/Kmeans/Resources/Input/clusters.txt
NUMBER_OF_REDUCERS=3
OUTPUT_DIR=~/MapReduce/Kmeans/Resources/Output
DELTA=100000000.0
MAX_ITERATIONS=10
DISTANCE=eucl

hadoop jar ${JAR_PATH} ${MAIN_CLASS} --input ${INPUT_FILE_PATH} \
--state ${STATE_PATH} \
--number ${NUMBER_OF_REDUCERS} \
--output ${OUTPUT_DIR} \
--delta ${DELTA} \
--max ${MAX_ITERATIONS} \
--distance ${DISTANCE}

# execute jar file
LAST_DIR="$(hadoop fs -ls -t -C ~/KMeans/Resources/Output | head -1)"

# print results
hadoop fs -cat "$LAST_DIR/part-r-[0-9][0-9][0-9][0-9][0-9]" | sort --numeric --key 1




# specify input parameters
JAR_PATH=~/MapReduce/Kmeans/executable_jar/kmeans_mapreduce.jar
MAIN_CLASS=Main
INPUT_FILE_PATH=/KMeans/Resources/Input/points.txt
STATE_PATH=/KMeans/Resources/Input/clusters.txt
NUMBER_OF_REDUCERS=3
OUTPUT_DIR=/KMeans/Resources/Output
DELTA=100000000.0
MAX_ITERATIONS=10
DISTANCE=eucl

hadoop jar ${JAR_PATH} ${MAIN_CLASS} --input ${INPUT_FILE_PATH} \
--state ${STATE_PATH} \
--number ${NUMBER_OF_REDUCERS} \
--output ${OUTPUT_DIR} \
--delta ${DELTA} \
--max ${MAX_ITERATIONS} \
--distance ${DISTANCE}



스파크 실행: $SPARK_HOME/sbin/start-all.sh
스파크 중지: $ $SPARTK_HOME/sbin/stop-all.sh

하둡 오류 시 아래의 절차로 복구
• 스파크종료:$SPARTK_HOME/sbin/stop-all.sh
• 하둡종료:$stop-all.sh
• 하둡데이터디렉토리삭제:$rm–r$HADOOP_HOME/hdfs/data
• 하둡재시작:$start-all.sh
• 하둡포맷:$hadoopnamenode–format
• 기존의하둡데이터는모두삭제됨
• jar파일복사
$ hadoop fs –mkdir /jar
$ hadoop fs -mkdir /jar/spark-jars
$ hadoop fs –put $SPARK_HOME/jars/* /jar/spark-jars/
• 스파크시작:$SPARK_HOME/sbin/start-all.sh




sudo rm Raffle.scala



sudo nano Raffle.scala
sudo scalac Raffle.scala
 scala Raffle.scala

스파크실행
start-all.sh
$SPARK_HOME/sbin/start-all.sh
$SPARK_HOME/sbin/stop-all.sh
Stop-all.sh

$SPARK_HOME/bin/spark-shell --master yarn



val sfpdDS=spark.read.option("inferSchema",true).csv("/sparkdata/sfpd/sfpd.csv").toDF("incidentnum","category","description","dayofweek","date","time","pddistrict","resolution","address","X","Y","paid").as[Incidents]

